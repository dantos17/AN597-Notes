---
title: "Module14 Notes"
author: "Dani Antos"
date: "October 29, 2017"
output: html_document
---

Previously, we've only used continuous variables as predictors, but you can also use discrete variables.
We can also call this an ANCOVA
```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
class(z$gender)
summary(z$gender)
plot(z$height ~ z$gender) #if you have gender as a character rather than factor, you'll get an error, we can also use as.factor()
m <- lm(data = z, height ~ gender)
summary(m) #we could also do a z or t test
levels(z$gender) #I'm a little fuzzy on what's happening here, something about the intercept being a factor instead of a number?
```
Okay so I think I get the whole factor thing. The equation here is basically:

height = beta1 + (beta0 x gender), where males = 1 and females = 0 (no matter how SEXIST that is)

- gives how much you would have to add to the baseline mean (female height) to get to the next categorical mean (male height)

The p value associated with the t statistic for beta1 is SUPER low, so we can assume that gender has an effect on height. 

```{r relevel()}
z$gender <- relevel(z$gender, ref = "Male")
m <- lm(data = z, height ~ gender)
summary(m)
p <- 1 - pf(276.9, df1 = 1, df2 = 998)
p
```

we're going to re-code the variable "major" into four levels (lol WUT I know)
```{r levels of major}
z$occupation <- "temp"
unique(z$major) #lists all the different majors in the dataset 
#levels() does the same thing alphabetically
levels(z$major)
row(data.frame(levels(z$major))) #I think this gives the location of each? unsure...seems a little counterintuitive to me
z$occupation[row(data.frame(levels(z$major))) %in% c(1, 2, 3, 5, 6, 14, 15, 
    16, 18, 21, 23)] <- "natural science"
z$occupation[row(data.frame(levels(z$major))) %in% c(7, 8, 12, 17, 19, 22)] <- "logistics"
z$occupation[row(data.frame(levels(z$major))) %in% c(4, 18, 20)] <- "engineering"
z$occupation[row(data.frame(levels(z$major))) %in% c(9, 10, 11, 13, 24, 25, 
    26)] <- "other"
z$occupation <- as.factor(z$occupation)
levels(z$occupation) #okay I think i get this but could never repeat it on my own
z$occupation <- relevel(z$occupation, ref = "natural science")
levels(z$occupation)
plot(data = z, zombies_killed ~ occupation) #plots our data by group and truly they all look identical to me
m <- lm(data = z, zombies_killed ~ occupation)
summary(m)
p <- 1 - pf(0.526, df1 = 3, df2 = 996)  # F test
p #no significant effect, I WAS RIGHT
```

##One way ANOVA
A one way ANOVA is just a regression with a single predictor (Chris says categorical here but I'm wondering if you could also say a continuous variable)
```{r one way ANOVA}
m <- aov(data = z, zombies_killed ~ occupation) #we can run the ANOVA in a single line of code
summary(m)
par(mfrow = c(2, 2))
plot(m)
```
I don't really know what's going on here. The table (?) shows the effect sizes for each coefficient and their standard errors  AS OPPOSED to the F statistic and p values, which show us that there are differences but not where those difference occur. 

- lm() arranges the effects in sequential contrasts, which is the default

- I really don't think my output is the same as what he's talking about, it doesn't make any sense to me

- ANOVA and simple regression aim to test the Ho, or that the means don't differ among the groups (mu1 = mu2 = mu3...). This is an extension of what we test for in z and t tests

- assumptions of ANOVA:

  1. samples are independent and identically distributed
  
  2. residuals are normally distributed
  
  3. within-group variances are similar across all groups
  
it's also helpful to have an equal number of cases in all groups
 
#Challenge
 Using the gibbon femurs data, is age and femur length significantly correlated? How do they differ for different categories of age?
```{r}
library(curl)
library(dplyr)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/gibbon-femurs.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
d$age <- factor(d$age, levels = c("inf", "juv", "subadult", "adult"))  #this reorders the age levels so that they're in order
head(d)
hist(d$femur.length)
qqnorm(d$femur.length)
plot(data = d, femur.length ~ age)  # boxplot with medians
means <- summarise(group_by(d, age), mean(femur.length))  # calculate average by group
points(1:4, means$`mean(femur.length)`, pch = 4, cex = 1.5)  # add means to plot #I'm going to need him to go over this
sds <- summarise(group_by(d, age), sd(femur.length))
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances are roughly equal (ratio of max/min is <2)
means.centered <- d$femur.length - means[as.numeric(d$age), 2]  # subtract relevant group mean from each data point #standardizing data by age, QQ plot looks normal
qqnorm(means.centered$`mean(femur.length)`)  # graphical tests for normality
par(mfrow = c(2, 2))
hist(d$femur.length[d$age == "inf"], main = "inf")
qqnorm(d$femur.length[d$age == "inf"])
hist(d$femur.length[d$age == "juv"], main = "juv")
qqnorm(d$femur.length[d$age == "juv"])
hist(d$femur.length[d$age == "subadult"], main = "subadult")
qqnorm(d$femur.length[d$age == "subadult"])
hist(d$femur.length[d$age == "adult"], main = "adult")
qqnorm(d$femur.length[d$age == "adult"])
#now we're starting the ANOVA, all previous steps were checking that the requirements were fulfilled
par(mfrow = c(1, 1))
plot(data = d, femur.length ~ age)
m <- aov(data = d, femur.length ~ age)  # femur length related to age #anova
summary(m)
m <- lm(data = d, femur.length ~ age)
summary(m) #simple regression
```

#Post-Hoc Test and Kruskal-Wallis Test (non-parametric)
We use post-hoc tests to find out which groups means are different from each other AFTER finding a significant F statistic
```{r post-hoc}
pairwise.t.test(d$femur.length, d$age, p.adj = "bonferroni")
m <- aov(d$femur.length ~ d$age)
posthoc <- TukeyHSD(m, "d$age", conf.level = 0.95)
posthoc  # all age-sex classes differ
m <- kruskal.test(data = d, femur.length ~ age)
m
d <- arrange(d, femur.length)  # use {dplyr} to sort by femur.length
d <- mutate(d, femur.rank = row(data.frame(d$femur.length)))  # use {dplyr} to add new variable of rank femur.length
m <- kruskal.test(data = d, femur.rank ~ age)
m #I don't think we have to add them, he just added femur.length as a column
```

**Kruskal-Wallis test** is nonparametric alternative to the one-way ANOVA, relaxes the need for normality in the data distribution

-we are testing the null hypothesis that the MEDIANS don't differ (rather than the means)

-converts the response variable into a set of ranks, then the p value is evaluated against Chi-square distribution

#Multiple factor ANOVA
Allows us to test several null hypotheses simultaneously: 

- each factor has no effect on the continuous response variable mean

- there are no interactive effects of sets of factors on response variable mean

- these have the same model descriptions and formulas as the one way ANOVA
```{r multiple factor ANOVA}
library(ggplot2)
means <- summarise(group_by(d, age, sex), mean(femur.length))  # first we calculate averages by combination of factors
means #but we're only taking the mean of one right?
sds <- summarise(group_by(d, age, sex), sd(femur.length))  # first we calculate averages by combination of factors
sds
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances in each group are roughly equal (ratio of max/min is <2)
p <- ggplot(data = d, aes(y = femur.length, x = sex)) + geom_boxplot() + facet_wrap(~age, 
    ncol = 4)  # and let's plot what the data look like
# p <- p + geom_point() # uncommenting this shows all points
p <- p + stat_summary(fun.y = mean, colour = "darkgreen", geom = "point", shape = 8, 
    size = 6)
p #we can see that there is a significant effect in age not 
summary(aov(data = d, femur.length ~ age))
summary(aov(data = d, femur.length ~ sex))
#now we do the multiple anova to consider the factors together
m <- summary(aov(data = d, femur.length ~ age + sex))
m
m <- aov(data = d, femur.length ~ age + sex + age:sex)  # : operator includes specific interaction terms
summary(m)
m <- lm(data = d, femur.length ~ age * sex)  # or using the lm() function...
summary(m)
interaction.plot(x.factor = d$age, xlab = "Age", trace.factor = d$sex, trace.label = "Sex", 
    response = d$femur.length, fun = mean, ylab = "Mean Femuur Length")
```

It looks like there is a significant main effect on each term as well as an interaction between the two categorical variables. 
```{r}
#I don't really understand what's going on here
m1 <- aov(data = d, femur.length ~ age * sex)
summary(m1)
m2 <- aov(data = d, femur.length ~ sex * age)
summary(m2)
m1 <- lm(data = d, femur.length ~ age * sex)
summary(m1)
m2 <- lm(data = d, femur.length ~ sex * age)
summary(m2)
table(d$sex, d$age)
```
m1 = variance within each age group that is explained by gender

m2 = variance within each gender that is explained by age

the *anova* is based on splitting the sum of squares so you get:

SS(total) = SS(a) + SS(b) + SS(ab) + SS(residual)

- aov() function uses the Type I Sum of Squares = gives greater emphasis to the first factor in the model, so only the residual variation is left for the other variables (use when you're trying to control for a variable)

- Type II Sum of Squares compares the effects of each group and assumes that interactions between them are minimal...usually a more appropriate test, especially when there is an unbalanced design (estimates everything at the same time)

- Type III Sum of Squares (also marginal sum of squares) is used when there is signicant interaction effect

we can use the Anova() function within the {car} package to run Type II or Type III Sum of Squares
```{r}
library(car)
m1 <- aov(data = d, femur.length ~ age + sex)
m1 <- Anova(m1, type = "II")
m1
m2 <- aov(data = d, femur.length ~ sex + age) #should this be m2?
m2 <- Anova(m2, type = "II")
m2 #would you get the same results by doing "sex*age" instead of "sex + age"?
m1 <- aov(data = d, femur.length ~ age * sex)
m1 <- Anova(m1, type = "III")
m1
m2 <- aov(data = d, femur.length ~ sex * age)
m2 <- Anova(m2, type = "III")
m2
```

#Chi-Squared Tests
we can use this to evaluate the distribution of observations across levels or one or more categorical variables

we need the Chi-Square stat! sum of (observed - expected)^2/expected 

#Challenge
we want to test the hypothesis that survivors of the zombie apocalypse are more likely to be natural science majors than expected by chance. So the null hypothesis is that the number of survivors for each major are equivalent (module says proportion but truly they are the same thing)
```{r}
obs.table <- table(z$occupation)  # returns the same as summary()
obs.table
exp.table <- rep(0.25 * length(z$occupation), 4)
exp.table
occupation.matrix <- data.frame(cbind(obs.table, exp.table, (obs.table - exp.table)^2/exp.table))
names(occupation.matrix) <- c("Oi", "Ei", "(Oi-Ei)^2/Ei")
occupation.matrix
X2 <- sum(occupation.matrix[, 3])
X2
p <- 1 - pchisq(X2, length(obs.table) - 1)
#we can reject the null
#OFC we can also do this in one line
chisq.test(x = obs.table, p = c(0.25, 0.25, 0.25, 0.25))  # here p is a vector of expected proportions... default is uniform
chisq.test(x = obs.table)
chisq.test(x = obs.table, p = c(0.38, 0.12, 0.23, 0.27))  # with a different set of expected proportions... fail to reject H0
```

Can we use chi square for two categorical variables? heck yeah. we do a **chi square test of independence**

- statistic is (O-E)^2/E sum for all cells in the table

- df = (number of rows-1) x (number of columns-1)

ex: relationship among survivors between gender and occupation
```{r}
obs.table = table(z$gender, z$occupation)
obs.table
mosaicplot(t(obs.table), main = "Contingency Table", col = c("darkseagreen", 
    "gray"))  # t function transposes the table
r <- rowSums(obs.table)  # row margins
r
c <- colSums(obs.table)  # column margins
c
nr <- nrow(obs.table)  # row dimensions
nr
nc <- ncol(obs.table)  # column dimensions
nc
exp.table <- matrix(rep(c, each = nr) * r/sum(obs.table), nrow = nr, ncol = nc, 
    dimnames = dimnames(obs.table))  # calculates the product of c*r and divides by total
exp.table
X2 <- sum((obs.table - exp.table)^2/exp.table)
X2
p <- 1 - pchisq(X2, df = (nr - 1) * (nc - 1))
p
chisq.test(x = obs.table)
```

